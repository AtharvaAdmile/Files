# Code:

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df=pd.read_csv("/content/Mall_Customers.csv")
print(df)

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

X = df.iloc[:, [3, 4]].values

x=df[['Annual Income (k$)','Spending Score (1-100)']]
print(x)

from sklearn.cluster import KMeans

wcss = []
for i in range(1, 11):
 kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
 kmeans.fit(X)
 wcss.append(kmeans.inertia_)
wcss

plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

kmean = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
kmean.fit(x)

x['cluster number'] = kmean.fit_predict(x)
x

# Scatter plot for each cluster
plt.scatter(X[kmeans.labels_ == 0, 0], X[kmeans.labels_ == 0, 1], s = 100, c = 'black', label = 'Cluster 1')
plt.scatter(X[kmeans.labels_ == 1, 0], X[kmeans.labels_ == 1, 1], s = 100, c = 'orange', label = 'Cluster 2')
plt.scatter(X[kmeans.labels_ == 2, 0], X[kmeans.labels_ == 2, 1], s = 100, c = 'yellow', label = 'Cluster 3')
plt.scatter(X[kmeans.labels_ == 3, 0], X[kmeans.labels_ == 3, 1], s = 100, c = 'pink', label = 'Cluster 4')
plt.scatter(X[kmeans.labels_ == 4, 0], X[kmeans.labels_ == 4, 1], s = 100, c = 'blue', label = 'Cluster 5')

# Plotting centroids
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'red', label = 'Centroids')

# Adding titles and labels
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')

# Adding legend
plt.legend()

# Display the plot
plt.show()

#Theory:

K-Means is a clustering algorithm used in unsupervised machine learning.
It groups data points into K distinct, non-overlapping clusters based on feature similarity.

Each cluster is represented by its centroid (the average of all points in that cluster).

How K-Means Works (Step-by-Step):
Step 1: Choose K
Decide how many clusters (K) you want the algorithm to find.

Step 2: Initialize Centroids
Randomly pick K data points from your dataset as the initial centroids (cluster centers).

Step 3: Assign Points to the Nearest Centroid
For each data point:

Calculate the distance to each centroid (typically using Euclidean distance).

Assign the point to the nearest centroid.

This forms K clusters.

Step 4: Recalculate Centroids
For each cluster:

Compute the mean of all data points assigned to it.

This new mean becomes the new centroid of the cluster.

Step 5: Repeat Until Convergence
Repeat Steps 3 and 4 until:

Centroids don’t change significantly.

Or the assignments of points to clusters don’t change.

Or you reach the maximum number of iterations.

Elbow method:

The Elbow Method involves plotting a graph of the Within-Cluster-Sum of Squared Errors (WCSS) versus number of clusters (K).
WCSS is the sum of squared distances between each point and the centroid of its cluster.
As K increases, WCSS decreases (because clusters are smaller and tighter).
But after a certain point, adding more clusters gives diminishing returns.

 How It Works (Step-by-Step):
1. Run K-Means for a range of K values (e.g., from 1 to 10).

2. Calculate WCSS for each value of K.

3. Plot K vs. WCSS on a line graph.

4.Look for the "elbow point" in the graph:

   This is the point where the decrease in WCSS becomes slower.
   It looks like an elbow in the graph.
   The K at this point is considered the optimal number of clusters.